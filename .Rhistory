filter(hpw < 20) %>%
summarise(hpw = mean(hpw, na.omit=T))
dat %>%
group_by(Hospital)
dat %>%
group_by(Hospital) %>%
filter(hpw < 20) %>%
summarise(hpw = mean(hpw, na.omit=T))
dat %>%
filter(Specialty=="PAED") %>%
summairse(dtwl = mean(as.numeric(`Daytime workload`)))
dat %>%
filter(Specialty=="PAED") %>%
summarise(dtwl = mean(as.numeric(`Daytime workload`)))
dat %>%
group_by(Speciality) %>%
summarise(dtwl = mean(as.numeric(`Daytime workload`)))
dat %>%
group_by(Specialty) %>%
summarise(dtwl = mean(as.numeric(`Daytime workload`)))
dat %>%
group_by(Specialty) %>%
summarise(dtwl = mean(as.numeric(`Daytime workload`)),
ostr = mean(as.numeric(`Workplace stress`)))
dat %>%
group_by(Specialty) %>%
summarise(dtwl = mean(as.numeric(`Daytime workload`)),
ostr = mean(as.numeric(`Workplace stress`),na.omit=T))
dat %>%
group_by(Specialty) %>%
summarise("daytime workload" = mean(as.numeric(`Daytime workload`)),
"overall stress" = mean(as.numeric(`Workplace stress`),na.omit=T))
dat %>% filter(Specialty=="SUR") %>% select(`Workplace stress`)
dat %>% filter(Specialty=="SUR") %>% select(`Workplace stress`) %>% mean()
dat %>% filter(Specialty=="SUR") %>% select(`Workplace stress`) %>% as.numeric() %>% mean()
dat %>% filter(Specialty=="SUR") %>% select(`Workplace stress`) %>% as.numeric() %>% mean(na.omit=T)
dat %>%
group_by(Specialty) %>%
summarise("daytime workload" = mean(as.numeric(`Daytime workload`)),
"workplace stress" = mean(as.numeric(`Workplace stress`),na.omit=T))
MrTrumpSpeeches <- read_delim("~/Downloads/mrtrump/MrTrumpSpeeches.csv",
"~", escape_double = FALSE, trim_ws = TRUE)
View(MrTrumpSpeeches)
install.packages("tm")  # for text mining
install.packages("SnowballC") # for text stemming
install.packages("wordcloud") # word-cloud generator
install.packages("RColorBrewer") # color palettes
# Load
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
t
filePath <- "http://www.sthda.com/sthda/RDoc/example-files/martin-luther-king-i-have-a-dream-speech.txt"
text <- readLines(filePath)
text
docs <- Corpus(VectorSource(text))
inspect(docs)
inspect(docs)
docs
docs
toSpace
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
?content_transformer
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
# docs <- tm_map(docs, stemDocument)
docs <- tm_map(docs, stemDocument)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
library(readr)
MrTrumpSpeeches <- read_delim("~/Downloads/mrtrump/MrTrumpSpeeches.csv",
"~", escape_double = FALSE, trim_ws = TRUE)
# Load
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
# Read the text file from internet
filePath <- "http://www.sthda.com/sthda/RDoc/example-files/martin-luther-king-i-have-a-dream-speech.txt"
text <- readLines(filePath)
# Load the data as a corpus
docs <- Corpus(VectorSource(text))
inspect(docs)
docs
docs
?content_transformer
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
# docs <- tm_map(docs, stemDocument)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
dtm <- TermDocumentMatrix(docs)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
library(readr)
MrTrumpSpeeches <- read_delim("~/Downloads/mrtrump/MrTrumpSpeeches.csv",
"~", escape_double = FALSE, trim_ws = TRUE)
# Load
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
# Read the text file from internet
filePath <- "http://www.sthda.com/sthda/RDoc/example-files/martin-luther-king-i-have-a-dream-speech.txt"
text <- readLines(filePath)
# Load the data as a corpus
docs <- Corpus(VectorSource(text))
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
# docs <- tm_map(docs, stemDocument)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
library(readr)
MrTrumpSpeeches <- read_delim("~/Downloads/mrtrump/MrTrumpSpeeches.csv",
"~", escape_double = FALSE, trim_ws = TRUE)
# Load
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
# Read the text file from internet
filePath <- "http://www.sthda.com/sthda/RDoc/example-files/martin-luther-king-i-have-a-dream-speech.txt"
text <- readLines(filePath)
# Load the data as a corpus
docs <- Corpus(VectorSource(text))
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
docs <- tm_map(docs, stemDocument)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
library(readr)
MrTrumpSpeeches <- read_delim("~/Downloads/mrtrump/MrTrumpSpeeches.csv",
"~", escape_double = FALSE, trim_ws = TRUE)
# Load
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
# Read the text file from internet
filePath <- "http://www.sthda.com/sthda/RDoc/example-files/martin-luther-king-i-have-a-dream-speech.txt"
text <- readLines(filePath)
# Load the data as a corpus
docs <- Corpus(VectorSource(text))
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
docs <- tm_map(docs, stemDocument)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
head(d, 10)
set.seed(123)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
docs <- Corpus(VectorSource(MrTrumpSpeeches$subtitles))
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, toSpace, "[")
docs <- tm_map(docs, toSpace, "\\[")
docs <- tm_map(docs, toSpace, "\\]")
docs
inspect(docs)
docs <- tm_map(docs, content_transformer(tolower))
+# Remove numbers
docs <- tm_map(docs, removeNumbers)
+# Remove numbers
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, stemDocument)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2", "music","the","and","have"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
docs <- tm_map(docs, stemDocument)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2", "music","the","and","have"))
stopwords("english")
docs <- tm_map(docs, content_transformer(tolower))
docs <- Corpus(VectorSource(MrTrumpSpeeches$subtitles))
Corpus(VectorSource(MrTrumpSpeeches$subtitles))
VectorSource(MrTrumpSpeeches$subtitles)
docs <- Corpus(VectorSource(MrTrumpSpeeches$subtitles))
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
docs <- tm_map(docs, toSpace, "\\[")
docs <- tm_map(docs, toSpace, "\\]")
inspect(docs)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2","pietÃ "))
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2"))
docs
docs <- tm_map(docs, stemDocument)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
docs <- tm_map(docs, removeWords, stopwords("english"))
MrTrumpSpeeches <- read_delim("~/Downloads/mrtrump/MrTrumpSpeeches.csv",
"~", escape_double = FALSE, trim_ws = TRUE, encoding="UTF-8")
F
MrTrumpSpeeches <- read.delim("~/Downloads/mrtrump/MrTrumpSpeeches.csv",
"~", escape_double = FALSE, trim_ws = TRUE)
MrTrumpSpeeches <- read.delim("~/Downloads/mrtrump/MrTrumpSpeeches.csv",
"~",fileEncoding = "utf-8")
MrTrumpSpeeches <- read.delim("~/Downloads/mrtrump/MrTrumpSpeeches.csv", header=F,
"~",fileEncoding = "utf-8")
MrTrumpSpeeches <- read.csv("~/Downloads/mrtrump/MrTrumpSpeeches.csv",sep='~',stringsAsFactors=F)
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
filePath <- "http://www.sthda.com/sthda/RDoc/example-files/martin-luther-king-i-have-a-dream-speech.txt"
docs <- Corpus(VectorSource(MrTrumpSpeeches$subtitles))
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
docs <- tm_map(docs, toSpace, "\\[")
docs <- tm_map(docs, toSpace, "\\]")
inspect(docs)
docs <- tm_map(docs, content_transformer(tolower))
+# Remove numbers
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, stemDocument)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
d$word
MrTrumpSpeeches$subtitles
"left" %in% MrTrumpSpeeches$subtitles
MrTrumpSpeeches$subtitles
MrTrumpSpeeches$subtitles[1]
"presidents" %in% MrTrumpSpeeches$subtitles[1]
knitr::purl("importdata.Rmd")
knitr::purl("importdata.Rmd")
knitr::purl("importdata.Rmd")
library(sparklyr)
library(dplyr)
sc <- spark_connect(master = "local", version = "2.1.0")
origclass <- c('integer','integer','character', 'integer', 'character', 'real', 'integer',
'character','real','integer','integer','integer','real','character','character','character','character',
'character','character','character','character', 'integer', 'integer','character','character' ,'character')
system.time(
origin <- spark_read_csv(
path = "/Users/erik/Desktop/project/data/origination/*.txt",
sc = sc, name = "origin", delimiter = "|", header=FALSE,
columns = origclass )
)
svcgclass <- c('character','integer','real','character', 'integer','integer','character','character',
'character','integer','real','real','integer', 'integer', 'character','integer','integer',
'integer','integer','integer','integer','real','real')
system.time(
month <- spark_read_csv(
path = "/Users/erik/Desktop/project/data/monthlyperformance/*.txt",
sc = sc, name = "month", delimiter = "|", header=FALSE,
columns = svcgclass)
)
length(svcgclass)
names(origin) <- c('fico','dt_first_pi','flag_fthb','dt_matr','cd_msa',"mi_pct",'cnt_units',
'occpy_sts','cltv','dti','orig_upb','ltv','int_rt','channel','ppmt_pnlty',
'prod_type','st', 'prop_type','zipcode','id_loan','loan_purpose',
'orig_loan_term','cnt_borr','seller_name','servicer_name', 'flag_sc')
names(month) <- c('id_loan','svcg_cycle','current_upb','delq_sts','loan_age','mths_remng',
'repch_flag','flag_mod', 'cd_zero_bal',
'dt_zero_bal','current_int_rt','non_int_brng_upb','dt_lst_pi','mi_recoveries',
'net_sale_proceeds','non_mi_recoveries','expenses', 'legal_costs',
'maint_pres_costs','taxes_ins_costs','misc_costs','actual_loss', 'modcost')
x <- c('id_loan','svcg_cycle','current_upb','delq_sts','loan_age','mths_remng',
'repch_flag','flag_mod', 'cd_zero_bal',
'dt_zero_bal','current_int_rt','non_int_brng_upb','dt_lst_pi','mi_recoveries',
'net_sale_proceeds','non_mi_recoveries','expenses', 'legal_costs',
'maint_pres_costs','taxes_ins_costs','misc_costs','actual_loss', 'modcost')
length(x)
svcgclass <- c('character','integer','real','character', 'integer','integer','character','character',
'character','integer','real','real','integer', 'integer', 'character','integer','integer',
'integer','integer','integer','integer','real','real')
length(svcgclass)
system.time(
month <- spark_read_csv(
path = "/Users/erik/Desktop/project/data/monthlyperformance/historical_data1_time_Q22000.txt",
sc = sc, name = "month", delimiter = "|", header=FALSE,
columns = svcgclass)
)
system.time(
month <- spark_read_csv(
path = "/Users/erik/Desktop/project/data/monthlyperformance/historical_data1_time_Q22000.txt",
sc = sc, name = "month", delimiter = "|", header=FALSE)
)
head(month)
month$V24
month$V23
month$V22
svcgfile_Qnyyyy <- read.table("/Users/erik/Desktop/project/data/monthlyperformance/historical_data1_time_Q22000.txt"", sep="|", header=FALSE,
colClasses=svcgclass)
## ------------------------------------------------------------------------
# origin names
names(origin) <- c('fico','dt_first_pi','flag_fthb','dt_matr','cd_msa',"mi_pct",'cnt_units',
'occpy_sts','cltv','dti','orig_upb','ltv','int_rt','channel','ppmt_pnlty',
'prod_type','st', 'prop_type','zipcode','id_loan','loan_purpose',
'orig_loan_term','cnt_borr','seller_name','servicer_name', 'flag_sc')
# monthly performance names
names(month) <- c('id_loan','svcg_cycle','current_upb','delq_sts','loan_age','mths_remng',
'repch_flag','flag_mod', 'cd_zero_bal',
'dt_zero_bal','current_int_rt','non_int_brng_upb','dt_lst_pi','mi_recoveries',
'net_sale_proceeds','non_mi_recoveries','expenses', 'legal_costs',
'maint_pres_costs','taxes_ins_costs','misc_costs','actual_loss', 'modcost')
svcgfile_Qnyyyy <- read.table("/Users/erik/Desktop/project/data/monthlyperformance/historical_data1_time_Q22000.txt", sep="|", header=FALSE,
colClasses=svcgclass)
head(svcgfile_Qnyyyy)
unique(svcgfile_Qnyyyy$v24)
unique(svcgfile_Qnyyyy$v23)
unique(svcgfile_Qnyyyy$v22)
unique(svcgfile_Qnyyyy$V1)
unique(svcgfile_Qnyyyy$V2)
unique(svcgfile_Qnyyyy$V3)
unique(svcgfile_Qnyyyy$V4)
unique(svcgfile_Qnyyyy$V5)
unique(svcgfile_Qnyyyy$V6)
unique(svcgfile_Qnyyyy$V20)
unique(svcgfile_Qnyyyy$V21)
unique(svcgfile_Qnyyyy$V22)
unique(svcgfile_Qnyyyy$V23)
unique(svcgfile_Qnyyyy$V24)
svcgclass <- c('character','integer','real','character', 'integer','integer','character','character',
'character','integer','real','real','integer', 'integer', 'character','integer','integer',
'integer','integer','integer','integer','real','real','character')
svcgfile_Qnyyyy <- read.table("/Users/erik/Desktop/project/data/monthlyperformance/historical_data1_time_Q22000.txt", sep="|", header=FALSE,
colClasses=svcgclass)
head(svcgfile_Qnyyyy)
unique(svcgfile_Qnyyyy$V24)
system.time(
month <- spark_read_csv(
path = "/Users/erik/Desktop/project/data/monthlyperformance/historical_data1_time_Q22000.txt",
sc = sc, name = "month", delimiter = "|", header=FALSE,
columns = svcgclass)
)
svcgclass <- c('character','integer','real','character', 'integer','integer','character','character',
'character','integer','real','real','integer', 'integer', 'character','integer','integer',
'integer','integer','integer','integer','real','real','character')
system.time(
month <- spark_read_csv(
path = "/Users/erik/Desktop/project/data/monthlyperformance/historical_data1_time_Q22000.txt",
sc = sc, name = "month", delimiter = "|", header=FALSE,
columns = svcgclass)
)
